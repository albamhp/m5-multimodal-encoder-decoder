{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise multimodal recognition: RGB-D scene recognition\n",
    "\n",
    "This exercise consists of three parts: two tutorials and the deliverable. The students must modify the code of the tutorial part, and write and discuss the results in the deliverable part that will be used to evaluate the exercise.\n",
    "\n",
    "If you are not familiar with jupyter notebooks please check __[this tutorial](https://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/What%20is%20the%20Jupyter%20Notebook.html)__ first.\n",
    "\n",
    "# Part 1 (tutorial): HHA baseline\n",
    "\n",
    "In this tutorial, you will use a pretrained convolutional network and replace the classifier for the target dataset using PyTorch. The code is loosely based on the __[PyTorch transfer learning tutorial](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)__. Just execute the code sequentially, paying attention to the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "from utils import imshow_unimodal, MEAN_DEPTH, STD_DEPTH\n",
    "\n",
    "plt.ion()  # interactive mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n",
    "\n",
    "We will use torchvision, torch.utils.data and RGBDutils packages for loading the\n",
    "data. The dataset is structured hierarchically in splits\\modalities\\classes (check the folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../../datasets/sunrgbd_lite\\\\train\\\\hha'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b81de6556505>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x, modality),\n\u001b[0;32m     32\u001b[0m                                           data_transforms[x])\n\u001b[1;32m---> 33\u001b[1;33m                   for x in partitions}\n\u001b[0m\u001b[0;32m     34\u001b[0m dataloaders = {x: data.DataLoader(image_datasets[x], batch_size=4,\n\u001b[0;32m     35\u001b[0m                                   shuffle=True, num_workers=4)\n",
      "\u001b[1;32m<ipython-input-8-b81de6556505>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     31\u001b[0m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x, modality),\n\u001b[0;32m     32\u001b[0m                                           data_transforms[x])\n\u001b[1;32m---> 33\u001b[1;33m                   for x in partitions}\n\u001b[0m\u001b[0;32m     34\u001b[0m dataloaders = {x: data.DataLoader(image_datasets[x], batch_size=4,\n\u001b[0;32m     35\u001b[0m                                   shuffle=True, num_workers=4)\n",
      "\u001b[1;32m~\\Documents\\STUDIES\\UNIVERSITY\\Master\\Master-Projects\\m5-multimodal-encoder-decoder\\venv\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[0;32m    207\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[0;32m    208\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m                                           target_transform=target_transform)\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\STUDIES\\UNIVERSITY\\Master\\Master-Projects\\m5-multimodal-encoder-decoder\\venv\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\STUDIES\\UNIVERSITY\\Master\\Master-Projects\\m5-multimodal-encoder-decoder\\venv\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[1;34m(self, dir)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../../datasets/sunrgbd_lite\\\\train\\\\hha'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(MEAN_DEPTH, STD_DEPTH)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(MEAN_DEPTH, STD_DEPTH)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(MEAN_DEPTH, STD_DEPTH)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Modalities\n",
    "modality = 'hha'  # Uncomment to switch to HHA modality\n",
    "\n",
    "# Path to the dataset\n",
    "data_dir = '/home/mcv/datasets/sunrgbd_lite'\n",
    "\n",
    "# Preparing dataset and dataloaders\n",
    "partitions = ['train', 'val', 'test']\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x, modality),\n",
    "                                          data_transforms[x])\n",
    "                  for x in partitions}\n",
    "dataloaders = {x: data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                  shuffle=True, num_workers=4)\n",
    "               for x in partitions}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in partitions}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "use_gpu = cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize a few images**\n",
    "\n",
    "Let's visualize a few images to get familiar with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "inputs, classes = inputs[0:4], classes[0:4]\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow_unimodal(out, title=[class_names[x] for x in classes])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model\n",
    "------------------\n",
    "\n",
    "Now, let's write a general function to train a model. Details:\n",
    "\n",
    "-  Uses Adam algorithm for gradient descent.\n",
    "-  Early stoping using best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), file=sys.stdout, desc=\"Training\"):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            print('Phase %s' % phase)\n",
    "            if phase == 'train':\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0] * inputs.size(0)\n",
    "                # running_loss += loss.data[0] * inputs.size(0) # Pytorch 0.4\n",
    "                running_loss += loss.data.item() * inputs.size(0)  # Pytorch 1.0\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.floats() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, a function to evaluate the model on a particular set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, partition, criterion):\n",
    "    since = time.time()\n",
    "\n",
    "    model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for data in dataloaders[partition]:\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # wrap them in Variable\n",
    "        if use_gpu:\n",
    "            inputs = Variable(inputs.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        # running_loss += loss.data[0] * inputs.size(0) # Pytorch 0.4\n",
    "        running_loss += loss.data.item() * inputs.size(0)  # Pytorch 1.0\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    test_loss = running_loss / dataset_sizes[partition]\n",
    "    test_acc = running_corrects.floats() / dataset_sizes[partition]\n",
    "\n",
    "    print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Tested in {:.0f}m {:.0f}s Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60, test_loss, test_acc))\n",
    "\n",
    "    return test_acc, test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network\n",
    "----------------------\n",
    "\n",
    "The architecture of the network is shown in the following figure:\n",
    "<img src=\"figures/rgb_network.png\" />\n",
    "\n",
    "The following code creates the RGB network by (downloading and) instantiating an AlexNet trained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = models.alexnet(pretrained=True)\n",
    "\n",
    "# You can visualize the network\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the training/fine tuning parameters\n",
    "----------------------\n",
    "\n",
    "The following code creates the optimization criterio and set per-layer training rates to better control the fine tuning and training process. We use a very simple model in which all layers are frozen except the last fully connected one, i.e. the classifier, so it should be easy to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_classes = len(class_names)\n",
    "c = model.classifier\n",
    "num_ftrs = c[6].in_features\n",
    "model.classifier = nn.Sequential(c[0], c[1], c[2], c[3], c[4], c[5], nn.Linear(num_ftrs, num_classes))\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "perlayer_optim = [\n",
    "    {'params': model.features[0].parameters(), 'lr': 0.00},  # conv1\n",
    "    {'params': model.features[3].parameters(), 'lr': 0.00},  # conv2\n",
    "    {'params': model.features[6].parameters(), 'lr': 0.00},  # conv3\n",
    "    {'params': model.features[8].parameters(), 'lr': 0.00},  # conv4\n",
    "    {'params': model.features[10].parameters(), 'lr': 0.00},  # conv5\n",
    "    {'params': model.classifier[1].parameters(), 'lr': 0.000},  # fc6\n",
    "    {'params': model.classifier[4].parameters(), 'lr': 0.000},  # fc7\n",
    "    {'params': model.classifier[6].parameters(), 'lr': 0.001}  # fc8\n",
    "]\n",
    "for param in itertools.chain(model.features[0].parameters(), model.features[3].parameters(),\n",
    "                             model.features[6].parameters(), model.features[8].parameters(),\n",
    "                             model.features[10].parameters(), model.classifier[1].parameters(),\n",
    "                             model.classifier[4].parameters(), model.classifier[6].parameters()):\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(perlayer_optim, lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the model\n",
    "-----------------\n",
    "\n",
    "It shouldn't take more than 2 mins to train with the GPU in the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "model = train_model(model, criterion, optimizer, None, num_epochs=25)\n",
    "\n",
    "# Evaluate\n",
    "train_acc, _ = evaluate_model(model, 'train', criterion)\n",
    "val_acc, _ = evaluate_model(model, 'val', criterion)\n",
    "test_acc, _ = evaluate_model(model, 'test', criterion)\n",
    "print('Accuracy. Train: %1.2f%% val: %1.2f%% test: %1.2f%%' %\n",
    "      (train_acc * 100, val_acc * 100, test_acc * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
